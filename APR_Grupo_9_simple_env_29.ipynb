{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad - Proyecto práctico\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1: Granizo, Mateo\n",
    "*   Alumno 2: Maiolo, Pablo\n",
    "*   Alumno 3: Miglino, Diego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PARTE 1** - Instalación y requisitos previos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "print(IN_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo_maiolo\\anaconda3\\envs\\miar_rl_3\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from rl.core import Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de acciones disponibles: 6\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "print(f\"Numero de acciones disponibles: {nb_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
    "model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=500000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                              value_max=1.0, value_min=0.1, value_test=0.01,\n",
    "                              nb_steps=150000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
    "               memory=memory, processor=processor,\n",
    "               nb_steps_warmup=50000, gamma=0.99, train_interval=4, delta_clip=1.0)\n",
    "dqn.compile(Adam(learning_rate=1e-4), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entrenamiento del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 250000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   10/10000 [..............................] - ETA: 59s - reward: 0.0000e+00  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo_maiolo\\anaconda3\\envs\\miar_rl_3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.0126\n",
      "15 episodes - episode_reward: 8.000 [3.000, 16.000] - ale.lives: 2.145\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.0141\n",
      "15 episodes - episode_reward: 9.533 [4.000, 19.000] - ale.lives: 2.083\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.0142\n",
      "14 episodes - episode_reward: 9.643 [5.000, 15.000] - ale.lives: 2.120\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.0168\n",
      "13 episodes - episode_reward: 12.077 [5.000, 23.000] - ale.lives: 2.175\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 0.0150\n",
      "13 episodes - episode_reward: 12.615 [5.000, 23.000] - ale.lives: 2.050\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 411s 41ms/step - reward: 0.0142\n",
      "15 episodes - episode_reward: 8.533 [3.000, 17.000] - loss: 0.007 - mae: 0.032 - mean_q: 0.045 - mean_eps: 0.670 - ale.lives: 1.898\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 398s 40ms/step - reward: 0.0126\n",
      "16 episodes - episode_reward: 9.250 [1.000, 22.000] - loss: 0.007 - mae: 0.052 - mean_q: 0.071 - mean_eps: 0.610 - ale.lives: 2.019\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0182\n",
      "11 episodes - episode_reward: 14.727 [4.000, 25.000] - loss: 0.007 - mae: 0.073 - mean_q: 0.099 - mean_eps: 0.550 - ale.lives: 2.025\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0138\n",
      "17 episodes - episode_reward: 9.294 [4.000, 20.000] - loss: 0.007 - mae: 0.090 - mean_q: 0.120 - mean_eps: 0.490 - ale.lives: 2.003\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0197\n",
      "11 episodes - episode_reward: 17.364 [5.000, 29.000] - loss: 0.007 - mae: 0.119 - mean_q: 0.160 - mean_eps: 0.430 - ale.lives: 1.993\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0170\n",
      "12 episodes - episode_reward: 14.333 [2.000, 38.000] - loss: 0.007 - mae: 0.141 - mean_q: 0.187 - mean_eps: 0.370 - ale.lives: 2.070\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0136\n",
      "13 episodes - episode_reward: 10.000 [2.000, 20.000] - loss: 0.008 - mae: 0.173 - mean_q: 0.226 - mean_eps: 0.310 - ale.lives: 2.085\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 413s 41ms/step - reward: 0.0177\n",
      "15 episodes - episode_reward: 12.467 [3.000, 32.000] - loss: 0.008 - mae: 0.201 - mean_q: 0.262 - mean_eps: 0.250 - ale.lives: 2.148\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 415s 42ms/step - reward: 0.0154\n",
      "12 episodes - episode_reward: 12.000 [2.000, 21.000] - loss: 0.008 - mae: 0.240 - mean_q: 0.310 - mean_eps: 0.190 - ale.lives: 2.030\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 419s 42ms/step - reward: 0.0165\n",
      "14 episodes - episode_reward: 12.071 [5.000, 26.000] - loss: 0.008 - mae: 0.270 - mean_q: 0.346 - mean_eps: 0.130 - ale.lives: 2.103\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 416s 42ms/step - reward: 0.0164\n",
      "12 episodes - episode_reward: 13.667 [3.000, 33.000] - loss: 0.008 - mae: 0.295 - mean_q: 0.377 - mean_eps: 0.100 - ale.lives: 2.056\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 413s 41ms/step - reward: 0.0177\n",
      "10 episodes - episode_reward: 17.700 [6.000, 29.000] - loss: 0.009 - mae: 0.332 - mean_q: 0.423 - mean_eps: 0.100 - ale.lives: 2.389\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 415s 41ms/step - reward: 0.0179\n",
      "14 episodes - episode_reward: 13.214 [4.000, 26.000] - loss: 0.008 - mae: 0.382 - mean_q: 0.483 - mean_eps: 0.100 - ale.lives: 2.062\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 425s 43ms/step - reward: 0.0166\n",
      "13 episodes - episode_reward: 11.538 [4.000, 21.000] - loss: 0.009 - mae: 0.429 - mean_q: 0.539 - mean_eps: 0.100 - ale.lives: 2.060\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 416s 42ms/step - reward: 0.0161\n",
      "16 episodes - episode_reward: 10.688 [1.000, 21.000] - loss: 0.009 - mae: 0.456 - mean_q: 0.572 - mean_eps: 0.100 - ale.lives: 2.088\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 417s 42ms/step - reward: 0.0163\n",
      "15 episodes - episode_reward: 10.467 [5.000, 17.000] - loss: 0.009 - mae: 0.478 - mean_q: 0.597 - mean_eps: 0.100 - ale.lives: 1.955\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 419s 42ms/step - reward: 0.0154\n",
      "15 episodes - episode_reward: 10.000 [0.000, 21.000] - loss: 0.009 - mae: 0.535 - mean_q: 0.667 - mean_eps: 0.100 - ale.lives: 2.050\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 417s 42ms/step - reward: 0.0156\n",
      "14 episodes - episode_reward: 12.214 [4.000, 21.000] - loss: 0.009 - mae: 0.563 - mean_q: 0.701 - mean_eps: 0.100 - ale.lives: 2.040\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      " 3261/10000 [========>.....................] - ETA: 5:00 - reward: 0.0147"
     ]
    }
   ],
   "source": [
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=25000)]\n",
    "callbacks += [FileLogger(log_filename, interval=1000)]\n",
    "\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=250000, log_interval=10000, visualize=False)\n",
    "\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test y visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de n episodios para calcular la recompensa final\n",
    "# NOTA: 'visualize=True' intentará abrir una ventana emergente.\n",
    "# Si no funciona, asegúrate de tener las dependencias de renderizado instaladas.\n",
    "# Para entornos Atari, prueba a ejecutar en tu terminal: pip install pyglet\n",
    "\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (miar_rl_3)",
   "language": "python",
   "name": "miar_rl_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
