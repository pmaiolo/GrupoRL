{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad - Proyecto práctico (Versión GPU Segura)\n",
    "> **VERSIÓN GPU SEGURA**: Configuración simplificada de GPU que evita errores de configuración mientras mantiene optimizaciones.\n",
    "*   Alumno 1: Granizo, Mateo\n",
    "*   Alumno 2: Maiolo, Pablo\n",
    "*   Alumno 3: Miglino, Diego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PARTE 1** - Instalación y requisitos previos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verificando entorno de ejecución...\")\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "  print(\"Entorno: Google Colab\")\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "  print(\"Entorno: Local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importando librerías...\")\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint, Callback\n",
    "from rl.core import Processor\n",
    "print(\"Librerías importadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Configurando GPU de forma segura...\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configuración simple y segura de GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU configurada exitosamente. GPUs disponibles: {len(gpus)}\")\n",
    "        print(f\"GPU principal: {gpus[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error configurando GPU: {e}\")\n",
    "        print(\"Continuando con configuración por defecto...\")\n",
    "else:\n",
    "    print(\"No se detectaron GPUs - usando CPU\")\n",
    "\n",
    "# Verificar que TensorFlow puede usar GPU\n",
    "print(f\"TensorFlow puede usar GPU: {tf.test.is_gpu_available()}\")\n",
    "print(f\"Dispositivos disponibles: {tf.config.list_physical_devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración optimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Configurando entorno optimizado...\")\n",
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Semillas para reproducibilidad\n",
    "np.random.seed(42)\n",
    "env.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "print(f\"Número de acciones disponibles: {nb_actions}\")\n",
    "print(f\"Espacio de observación: {env.observation_space.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Definiendo procesador optimizado...\")\n",
    "class OptimizedAtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3\n",
    "        # Procesamiento eficiente\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # Normalización eficiente\n",
    "        processed_batch = batch.astype('float32') / 255.0\n",
    "        return processed_batch - 0.5\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        # Sin clipping para Space Invaders - mejor rendimiento\n",
    "        return reward\n",
    "\n",
    "print(\"Procesador optimizado definido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Red neuronal optimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Construyendo red neuronal optimizada...\")\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "\n",
    "# Arquitectura optimizada pero estable\n",
    "model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
    "model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "# Capa adicional para mejor representación\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Capas densas optimizadas\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "\n",
    "print(model.summary())\n",
    "print(f\"Parámetros totales: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuración DQN optimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Configurando agente DQN optimizado...\")\n",
    "\n",
    "# Memoria optimizada pero segura\n",
    "memory = SequentialMemory(limit=750000, window_length=WINDOW_LENGTH)\n",
    "processor = OptimizedAtariProcessor()\n",
    "\n",
    "# Política de exploración optimizada\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                              value_max=1.0, value_min=0.1, value_test=0.05,\n",
    "                              nb_steps=1000000)\n",
    "\n",
    "# Configuración DQN optimizada\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
    "               memory=memory, processor=processor,\n",
    "               nb_steps_warmup=50000,\n",
    "               gamma=0.995,  # Ligeramente mayor para mejor planificación\n",
    "               train_interval=4, \n",
    "               batch_size=32,\n",
    "               delta_clip=1.0,\n",
    "               target_model_update=10000,\n",
    "               enable_double_dqn=True,\n",
    "               enable_dueling_network=False)\n",
    "\n",
    "# Optimizador mejorado\n",
    "optimizer = Adam(learning_rate=2e-4)  # Ligeramente más alto\n",
    "dqn.compile(optimizer, metrics=['mae'])\n",
    "\n",
    "print(\"Agente DQN optimizado configurado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Callbacks optimizados y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Definiendo callbacks optimizados...\")\n",
    "\n",
    "class OptimizedMilestoneRecorder(Callback):\n",
    "    def __init__(self, env, video_folder='videos/', milestones=[40, 60, 80, 100, 150]):\n",
    "        super(OptimizedMilestoneRecorder, self).__init__()\n",
    "        self.env = env\n",
    "        self.video_folder = video_folder\n",
    "        self.milestones = sorted(milestones)\n",
    "        self.achieved_milestones = []\n",
    "        self.best_reward = -float('inf')\n",
    "        self.episode_rewards = []\n",
    "        self.recent_rewards = []\n",
    "        \n",
    "        print(f\"[Callback] OptimizedMilestoneRecorder inicializado. Hitos: {self.milestones}\")\n",
    "        if not os.path.exists(video_folder):\n",
    "            print(f\"[Callback] Creando carpeta de videos: {video_folder}\")\n",
    "            os.makedirs(video_folder)\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        reward = logs.get('episode_reward')\n",
    "        if reward is None:\n",
    "            return\n",
    "        \n",
    "        self.episode_rewards.append(reward)\n",
    "        self.recent_rewards.append(reward)\n",
    "        \n",
    "        # Mantener solo las últimas 100 recompensas\n",
    "        if len(self.recent_rewards) > 100:\n",
    "            self.recent_rewards.pop(0)\n",
    "        \n",
    "        avg_reward = np.mean(self.recent_rewards)\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if reward > self.best_reward:\n",
    "            self.best_reward = reward\n",
    "            best_weights_filename = f'dqn_{env_name}_best_gpu_safe_weights.h5f'\n",
    "            print(f'\\n[Callback] ¡Nuevo mejor resultado! Recompensa: {reward:.2f} (Promedio: {avg_reward:.2f})')\n",
    "            print(f'[Callback] Guardando mejor modelo: {best_weights_filename}')\n",
    "            self.model.save_weights(best_weights_filename, overwrite=True)\n",
    "        \n",
    "        # Verificar hitos\n",
    "        for milestone in self.milestones:\n",
    "            if reward >= milestone and milestone not in self.achieved_milestones:\n",
    "                print(f'\\n[Callback] ¡Hito alcanzado! Recompensa: {reward:.2f} >= {milestone}')\n",
    "                \n",
    "                weights_filename = f'dqn_{env_name}_weights_gpu_safe_reward_{milestone}.h5f'\n",
    "                print(f\"[Callback] Guardando pesos del hito: {weights_filename}\")\n",
    "                self.model.save_weights(weights_filename, overwrite=True)\n",
    "                \n",
    "                # Intentar grabar video\n",
    "                self._record_video(milestone)\n",
    "                self.achieved_milestones.append(milestone)\n",
    "                break\n",
    "        \n",
    "        # Progreso cada 50 episodios\n",
    "        if episode % 50 == 0 and episode > 0:\n",
    "            print(f'\\n[Progreso] Episodio {episode}: Recompensa actual: {reward:.2f}, '\n",
    "                  f'Promedio últimos 100: {avg_reward:.2f}, Mejor: {self.best_reward:.2f}')\n",
    "    \n",
    "    def _record_video(self, milestone):\n",
    "        video_path = os.path.join(self.video_folder, f'milestone_gpu_safe_reward_{milestone}')\n",
    "        print(f\"[Callback] Intentando grabar video en: {video_path}\")\n",
    "        try:\n",
    "            video_env = None\n",
    "            \n",
    "            # Intentar diferentes métodos de grabación\n",
    "            try:\n",
    "                from gym.wrappers.record_video import RecordVideo\n",
    "                video_env = RecordVideo(self.env, video_path)\n",
    "                print(f\"[Callback] Usando gym.wrappers.record_video.RecordVideo\")\n",
    "            except ImportError:\n",
    "                try:\n",
    "                    video_env = gym.wrappers.RecordVideo(self.env, video_path)\n",
    "                    print(f\"[Callback] Usando gym.wrappers.RecordVideo\")\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        video_env = gym.wrappers.Monitor(self.env, video_path, force=True)\n",
    "                        print(f\"[Callback] Usando gym.wrappers.Monitor\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            \n",
    "            if video_env is not None:\n",
    "                self.model.test(video_env, nb_episodes=1, visualize=False)\n",
    "                video_env.close()\n",
    "                print(f\"[Callback] Video guardado exitosamente\")\n",
    "            else:\n",
    "                print(f\"[Callback] No se pudo grabar video, continuando...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[Callback] Error al grabar video: {e}\")\n",
    "\n",
    "# Configurar archivos de salida\n",
    "log_filename = f'dqn_{env_name}_log_gpu_safe.json'\n",
    "weights_filename = f'dqn_{env_name}_weights_gpu_safe.h5f'\n",
    "\n",
    "callbacks = [\n",
    "    FileLogger(log_filename, interval=1000),\n",
    "    OptimizedMilestoneRecorder(env),\n",
    "    ModelIntervalCheckpoint(f'dqn_{env_name}_gpu_safe_checkpoint_{{step}}.h5f', interval=250000)\n",
    "]\n",
    "\n",
    "print(\"Callbacks optimizados configurados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INICIANDO ENTRENAMIENTO OPTIMIZADO GPU SEGURO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Pasos de entrenamiento: 1,750,000\")\n",
    "print(f\"Pasos de calentamiento: 50,000\")\n",
    "print(f\"Learning rate: 2e-4\")\n",
    "print(f\"Gamma: 0.995\")\n",
    "print(f\"Batch size: 32\")\n",
    "print(f\"Memoria: 750,000 experiencias\")\n",
    "print(f\"Target update: cada 10,000 pasos\")\n",
    "print(f\"Sin clipping de recompensas\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Entrenamiento optimizado\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000, visualize=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENTRENAMIENTO COMPLETADO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Guardar modelo final\n",
    "print(f\"Guardando pesos finales en: {weights_filename}\")\n",
    "dqn.save_weights(weights_filename, overwrite=True)\n",
    "print(\"Pesos finales guardados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Análisis de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analizando resultados del entrenamiento...\")\n",
    "try:\n",
    "    with open(log_filename, 'r') as f:\n",
    "        log_data = json.load(f)\n",
    "    \n",
    "    episode_rewards = log_data.get('episode_reward', [])\n",
    "    nb_episode_steps = log_data.get('nb_episode_steps', [])\n",
    "    \n",
    "    if episode_rewards and nb_episode_steps:\n",
    "        # Calcular estadísticas\n",
    "        max_reward = max(episode_rewards)\n",
    "        mean_reward = np.mean(episode_rewards)\n",
    "        std_reward = np.std(episode_rewards)\n",
    "        \n",
    "        # Promedio móvil\n",
    "        window_size = 100\n",
    "        moving_avg = []\n",
    "        for i in range(len(episode_rewards)):\n",
    "            start_idx = max(0, i - window_size + 1)\n",
    "            moving_avg.append(np.mean(episode_rewards[start_idx:i+1]))\n",
    "        \n",
    "        print(f\"\\nEstadísticas del entrenamiento:\")\n",
    "        print(f\"Recompensa máxima: {max_reward:.2f}\")\n",
    "        print(f\"Recompensa promedio: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "        print(f\"Número total de episodios: {len(episode_rewards)}\")\n",
    "        print(f\"Promedio últimos 100 episodios: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "        \n",
    "        # Gráfico principal\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        steps_cumsum = np.cumsum(nb_episode_steps)\n",
    "        plt.plot(steps_cumsum, episode_rewards, alpha=0.3, color='lightblue', label='Recompensa por episodio')\n",
    "        plt.plot(steps_cumsum, moving_avg, color='darkblue', linewidth=2, label=f'Promedio móvil ({window_size} episodios)')\n",
    "        plt.axhline(y=40, color='orange', linestyle='--', alpha=0.7, label='Objetivo básico (40)')\n",
    "        plt.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='Objetivo intermedio (80)')\n",
    "        plt.axhline(y=150, color='green', linestyle='--', alpha=0.7, label='Objetivo avanzado (150)')\n",
    "        plt.title('Progreso del Entrenamiento DQN GPU Seguro - Space Invaders', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Pasos de Entrenamiento')\n",
    "        plt.ylabel('Recompensa del Episodio')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Análisis de convergencia\n",
    "        if len(episode_rewards) > 500:\n",
    "            first_half = episode_rewards[:len(episode_rewards)//2]\n",
    "            second_half = episode_rewards[len(episode_rewards)//2:]\n",
    "            improvement = np.mean(second_half) - np.mean(first_half)\n",
    "            print(f\"\\nAnálisis de convergencia:\")\n",
    "            print(f\"Promedio primera mitad: {np.mean(first_half):.2f}\")\n",
    "            print(f\"Promedio segunda mitad: {np.mean(second_half):.2f}\")\n",
    "            print(f\"Mejora: {improvement:.2f} ({improvement/np.mean(first_half)*100:.1f}%)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No se encontraron datos de recompensa en el archivo de log.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"El archivo de log '{log_filename}' no fue encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al procesar el archivo de log: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluación final del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN FINAL DEL AGENTE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cargar el mejor modelo\n",
    "best_weights_file = f'dqn_{env_name}_best_gpu_safe_weights.h5f'\n",
    "if os.path.exists(best_weights_file):\n",
    "    print(f\"Cargando mejor modelo: {best_weights_file}\")\n",
    "    dqn.load_weights(best_weights_file)\n",
    "else:\n",
    "    print(f\"Cargando modelo final: {weights_filename}\")\n",
    "    dqn.load_weights(weights_filename)\n",
    "\n",
    "print(\"\\nEvaluando agente con 20 episodios...\")\n",
    "test_results = dqn.test(env, nb_episodes=20, visualize=False)\n",
    "\n",
    "print(f\"\\nResultados de la evaluación:\")\n",
    "print(f\"Recompensa promedio: {np.mean(test_results.history['episode_reward']):.2f}\")\n",
    "print(f\"Recompensa máxima: {np.max(test_results.history['episode_reward']):.2f}\")\n",
    "print(f\"Recompensa mínima: {np.min(test_results.history['episode_reward']):.2f}\")\n",
    "print(f\"Desviación estándar: {np.std(test_results.history['episode_reward']):.2f}\")\n",
    "\n",
    "# Mostrar distribución de recompensas de evaluación\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(test_results.history['episode_reward'], bins=10, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "plt.axvline(np.mean(test_results.history['episode_reward']), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Media: {np.mean(test_results.history[\"episode_reward\"]):.2f}')\n",
    "plt.title('Distribución de Recompensas en Evaluación Final', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Recompensa')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN COMPLETADA\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
